---
author: Sukhdeep Singh
categories:
  - Kafka
date: 23 May, 21
description: "In this article we'll discuss how Kafka is so resilient. Yes, you got me right. In this article, we'll discuss 'Kafka Architecture."
post_status: publish
post_type: post
tags:
  - Guide
  - tutorials
  - Kafka
thumbnailUrl: /apache-kafka-architecture-and-essential-components.jpg
title: Apache Kafka Architecture and Essential Components

nextPostLink: the-dummies-guide-to-kafka-operations
nextPostTitle: "The Dummiesâ€™ Guide To Kafka Operations"
nextPostImg: /the-dummies-guide-to-kafka-operations.png
prevPostLink: learning-apache-kafka
prevPostTitle: "Learning Apache Kafka Is Not Difficult Now"
prevPostImg: /learning-apache-kafka.jpg
---

_**We already discussed why Kafka is known as a fault-tolerant, distributed, high-performance messaging system. Let's now discuss 'how' Kafka is so resilient. Yes, you got me right. In this article, we'll discuss 'Kafka Architecture'. **_

Here, we'll discuss various components of Kafka, the role they play, and how they communicate. I tried to keep the explanations as simple as possible, so the theory shouldn't be overwhelming.

If you don't know about Kafka yet, then I'll strongly suggest you go through my [first article on Kafka](/blog/learning-apache-kafka-is-not-difficult-now).

## A simplistic view of Kafka architecture

Before moving deep into Kafka, let's first get an overview of Kafka's
architecture. That will give us a broad idea of the essential component. Then
we'll dig into details of these components along with others. I believe this
is the easiest approach to understand Kafka's architecture.

Just keep in mind
that this isn't the complete architecture of Kafka. We'll revisit this once
all the components discussed in detail. However, this diagram is enough to let
you start.
![Simplistic-Architecture-of-Kafka](/Simplistic-Architecture-of-Kafka.png)

- **Brokers** are the central system. It's the Kafka process that processes the messages.
- **Topics** : messages are grouped into _topics_. _Producers_ publish to the _topics_ and _consumers_ subscribe to the _topic_ which they are interested in. Each broker can have multiple partitions.
- **Partition** : Messages are stored in partitions in the form of a byte array. They are identified by **offsets** in the partition. Each broker can handle one or more partition.
- A producer can write to multiple partitions. **Combination of Topics, and partition helps the producer to identify where to place the data**

## Kafka Components

We already got an idea of Kafka architecture. Now let's dig a little deeper
into the various Kafka components.

### Messages

Messages are the basic entity around which the whole Kafka system works. They
carry the information, which the producer wants to convey to the world by
posting them to the broker/topic. The subscriber subscribes to the topic, and
reads the messages.

> Also note that each message is stored as an array of bytes.

Each message can also carry a **key** , which is used by _Kafka_ to store the
message in a particular partition.

#### Significance of Key

If the message has no _key_ , then the message can be written to any
partition. So ordering is not decided in that case. Some people say it's
round-robin. However, in my experiments, I didn't find any such order too.

However, if you provide a _key_ along with a message, then the following
formula is used by Kafka to decide the partition number:

_partition number = hash(key) total_number_of_partitions_

So if you want to write all of your
messages to a single partition, then provide them with the same key. However,
if you want to distribute your messages to different partitions, then provide
them with a different key.

#### Design criteria while deciding about key

There is an important aspect that needs to be kept in mind while selecting a
key.

If you want to distribute your messages to different partitions, then I'd
suggest you do it on some grouping criteria. **Messages which belong to the
same group should reside in the same partition**. Otherwise, you would get an
unexpected result.

To understand this, take the example of following diagram.
Here, the _Producer_ sends "hello" to the subscriber/consumer. Now, suppose
you use same key for _h, e_ , and _l_ and a different key for character _l_ ,
and _o_ . Because of this, they might fall into different partitions:
character _h, e_ , and _l_ to partition 2, and character _l_ , and _o_ to
partition 1. This is where the fun starts ðŸ¤¡ .

Now, we know that each partition
maintains its own offset. So it can be that _consumer_ reads _offset 0_ of
_partition 1_ before reading _offset 1_ of _partition 2_. This can change the
whole of the world. The resulting word would be _hlelo_. ( _This is not a
typo. It's real_ ðŸ˜‰)
![Message-with-different-keys](/Message-with-different-keys.png)
Therefore, use different keys only when messages
belong to different groups. Few of such examples could be _soccer_ messages
with a single key, _marathon_ under different key, etc. Remember, the broad
category is still the same: _sport_. If a broad category is going to be
different, then create a new _Topic_ itself.

I hope this should be clear now ðŸ™‚

### Topics

You can call topics as a group of messages which belong to the **same
category**. This is where _producers_ publish their messages and _consumers_
subscribe to read their messages.

<section class="message">
  <header>
    <i></i>Still, Confused?
  </header>

You can link a _topic_ to a _table_ in a database. Even if it's _not_ exactly like a table. However, that helps to understand the concept of a topic.

</section>

In a Kafka Cluster, there can
be multiple _topics_ with each topics catering to different category of
messages such as _sports, education or technology_.

### Partitions

Each topic is further divided into _partitions_. It not only helps to improve
parallelism but also to support _sub-categories_ of messages as we discuss
under messages section.

The most interesting part about partition is that you
can **create a partition on any Kafka server/broker**. You don't have to
create all partitions on the same server, which belongs to a single topic.
This also means that the _<span id="text-highlight">same topic can be spanned to multiple servers</span>_.

This is one of the factors that makes Kafka really too fast as multiple machines
serving to a single producer. Obviously, the consumer's load is also split
across multiple servers.

#### Partition offsets

_Partitions_ label each received _bytes_ with a unique sequence id called an
**offset**. Offsets are used when producers want to publish the data or
consumers want to read the data. Each party is assigned different offsets.
Even consumers have their own offsets. This is really helpful when one
consumer is slower than other consumers. This also helps when a consumer
crashes or has to take down because of maintenance. At that time having a
different offset helps it to resume its read operation.

#### Replicas of a partition

We already discussed that a new partition can be created on any server/broker.

In addition, you can also create any number of replicas of an existing
partition on other servers. However, you can't have a replica in the same
broker. This is only possible through different brokers. Thus you can say that
the replication factor depends on the number of brokers in the Kafka Cluster.

So does that mean, now the producer has to write to replicas too ðŸ¤” ? How
consumers will decide which partition to connect to when there are multiple
copies? The answer to these questions is given in the next section.

Partitions have one leader and zero or more followers. Each partition is replicated
across a configurable number of servers for fault tolerance. Replicas are
nothing but backups of a partition. Replicas are never read or write data.
They are used to prevent data loss.

#### Leader and Follower partitions

One thing to understand here is that replicas are just backups. They never
work at the front end. There is only one partition who remains at front, and
that's called **leader**. It's the _leader_ who serves producers and
subscribers. Obviously, there can't be more than one leader as that will
create another confusion then.

Other replicas working as backups just follow the content of the _leader_ partition. That' why they are named **followers**.

Here, comes another interesting story. When _leader_ crashes, then one of the
_follower_ takes the role of a _leader_ , and continue the work started by
_leader_.

> This makes Kafka fault resilient. If one server goes down, other can take over its job.

I feel now the analogy should be clear that for a single partition, there is
only one leader and zero or more followers.

### Brokers and Clusters

**Brokers** are the real Kafka _processes_ that act between publishers and
subscribers. All other entities such as topics or partitions work under the
hood of a broker. You can say this is the core of the Kafka system.

If you have more than one brokers in a system, then that makes a **Kafka Cluster**.
In short, Kafka cluster is comprised of a number of _brokers_ and in each
cluster, there is one controller. The main task of this controller is to
handle partitions assignments to brokers.
![Kafka-Cluster](/Kafka-Cluster.png)

Few points to note down:

- You can run multiple Brokers on the same machine or on a different machines.
- You can add any number of Brokers to a Kafka cluster without impacting others.
- Network latency should not be more than 15ms between various brokers in a cluster.

#### Brokers and partitions

Looking at the relationship between brokers and partitions, there are three
scenarios possible:

- If number of brokers and partitions _in a topic_ are same then each broker will get one partition of that particular topic.
- If the number of brokers are more than number of partitions, then extras brokers will not have any partitions for that topic.
- However, if number of partitions are more than number of brokers, then a few brokers will have more than one partition. Nevertheless it's possible but this isn't recommended by Kafka because of **uneven load distribution**.

<section class="message">
  <header>
    <i></i>Remember this
  </header>

In a Kafka cluster, there are multiple brokers. Each broker can have multiple topics. These topics contain a set of partitions. This is where messages are stored.

</section>

### Producers

Producers are the entities that produce the data and sends that to Kafka
brokers. They're used interchangeably with _publishers_. While publishing the
message, they have to specify the _topic_ name where they want to publish the
message. Optionally, they can also specify the _key_ parameter. This way they
can write the messages with the same key to the same partition. We already
discussed this in detail under [message keys](/blog/apache-kafka-architecture-and-essential-components/#significance-of-key).

<span id="text-highlight">
  Moreover, a publisher can send the data to any number of topics
</span>
.

### Consumers

As their name implies, they're the ones who consume the data. But before doing
that they have to subscribe to the respective topic. Then, whenever any
message will be published to that topic, they would be informed about that.
Subsequently, they can read the data.

#### Essential Features of the consumers

- Consumer have to provide their offset along with the _topic name_ and _partition number_ while reading the data.
- The offset is increased by the number of bytes read every time.
- Offsets are stored in special topic inside _Kafka_ : `_consumer_offset`.
- This makes consumers handle scenarios like system crash or restart. They can retrieve their last successful read offset, and continue from there.

- data read from the same partitions are **always in the same order** in which they're written to.

- Just like producers, consumers can subscribe and **read from multiple topics**.
- The consumer will **poll the Kafka** in a regular interval (like 100 ms) for new messages.
- The consumers can rewind or skip to any point in a partition simply by supplying an offset value.

### Consumer Group

Every consumer is part of a _consumer group_. It's up to you how you want to
group consumers. You can groups them on the basis of a topic, means all
consumers interested in the same topic can be grouped together. Another option
could be to group them as per their use case such as all consumers involved in
data analysis are grouped together irrespective of whatever category of data.
There are n-number of choices here. Thus it all depends on your design
decision.

A partition can only be connected to a single consumer in a group.
Connecting the same partition to multiple consumers in a group isn't possible
(unless you change at the lower level, which isn't advisable). However, the
reverse is true that the same consumer can be connected to multiple
partitions. I tried to explain these scenarios with multiple figures.

![1-Consumer-group-2-partitions-and-2-consumers](/1-Consumer-group-2-partitions-and-2-consumers.png)
This is a 1-1
relationship between partition and consumer group. So that's perfectly fine.
![1-Consumer-group 2-partitions and 2-consumers: wrong-configuration](/1-Consumer-group-2-partitions-and-2-consumers-wrong-configuration2.png)
Above is the same consumer from the same group is connected to multiple partitions, so that's
wrong.
![1-Consumer-group-2-partitions-and-3-consumers](/1-Consumer-group-2-partitions-and-3-consumers.png)
Here, we've extra consumers than the number of
partitions available. Hence, sitting idle. This kind of configuration is
useful when you want to handle the failsafe situation in case one consumer
goes down. Another use case could be when you anticipate that there can be
more partitions in the future.
![1-Consumer-group-3-partitions-and-2-consumers](/1-Consumer-group-3-partitions-and-2-consumers.png)
Here, consumers are lesser than the number of
partitions. Therefore, one consumer has to handle two partitions together.
![2-Consumer-groups-2-partitions-and-2-consumers](/2-Consumer-groups-2-partitions-and-2-consumers.png)
Here partitions are connected to consumers from different groups. But no two partitions are
connected to the same consumer from the same group. So this configuration is
also fine.

<section class="message">
  <header>
    <i></i>Failsafe
  </header>

As we discussed, a partition cannot be assigned to more than one consumer in a group. But if that consumer crashes, then the partition is assigned to another consumer from the same group.

The group leader is responsible to do this partition re-assignment.

</section>

The tool `kafka-consumer- group.sh` helps to get details of consumer group:

```shell
# Inside a Kafka broker container
bin/kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group order-group --members --verbose
```

### Zookeeper

Zookeeper is another open-source project. The most two important roles played
here by Zookeeper are:

- to manage coordination between different brokers,
- notification to producer/consumer when a broker joins/leaves the cluster.

If you need more detail on zookeeper, then you can refer to [this link](https://www.tutorialspoint.com/zookeeper/index.htm). We'll not cover more detail here as that's not required ðŸ™‚ .

<section class="message warning">
  <header>
    <i></i>Note
  </header>

Earlier, Zookeeper also used to manage consumer offsets. However, recent versions of Kafka have taken over that work.

Moreover, <span id="text-highlight">planning is going on to make Kafka independent of Zookeeper altogether</span>. That's another reason that I didn't find this topic to be important enough to touch.

</section>

### Retention policy

Each record is stored on a disk in Kafka. So if these records are not deleted,
you can run out of disk space. So Kafka offers a feature of the _retention
policy_. Under this policy, messages are retained either for a specific period
of time(days/weeks/months) or till they reached a configured size (in bytes).
Once they cross any of these limits, they are deleted from the system.

By default, the retention time is _7 days_. However, you're free to change it as
per your design requirements. You can even configure different topics with
different retention policies where few topics want to be kept longer while a
few for just a few hours.

[Refer to Kafka documentation](https://kafka.apache.org/documentation/#brokerconfigs_log.retention.bytes) on how to configure these parameters.

> Kafka store log data in its `log.dir` and topic maps to subdirectories in this log directory.

## Message replications and In-Sync replica(ISR)

We already discussed how partitions are replicated across multiple brokers.
Now the question is how messages are passed/replicated to those partitions.

Let's first see the sequence diagram of how replicas happen when a message is
published by a producer.
![Partition-Replication-Message-Sequence-Diagram](/Partition-Replication-Message-Sequence-Diagram.png)
Whenever a message is published by the producer, it is sent
to the _Leader_ partition. From there, it is polled by _follower partitions_.
After successful pull by the followers, they replied back with acknowledgment.
Finally, the _leader_ partition sends the acknowledgment to the Producer.

However, it is up to the producer how does it want to get the response back:

- Does it want to wait until the _leader_ receives _ack_ from all the **In-Sync Replicas** or
- is it just interested to receive the ack from the leader as soon as the leader receives the packet
- The third option is to send the packet without worrying about the destiny of the packet.

Whatever way publisher is interested, messages can't be said **committed**
until message replication is done to all **In-Sync Replicas**.

**In-Sync Replicas** or **ISR** are those replicated partitions, whose messages are _in
sync_ with their leader. By default, the sync time allowed is 10 seconds.
However, if a replicated partition lags behind this time, then it is removed
from the ISR list.

You can configure the number of ISRs through
`min.insync.replicas` the configuration parameter. It specifies the minimum
number of replicas that must acknowledge a write in order to consider the
write to be successful.

When ISR is successful, then only a message is
considered committed. Only these committed messages are supplied to consumers.

## Revisiting Kafka Architecture

Now, we've discussed all the essential components of Kafka and their feature.
I feel this is the right time to revisit a more detailed diagram of Kafka
Architecture.
![Kafka-Architecture-Detailed](/Kafka-Architecture-Detailed.png)
I believe now it shouldn't be difficult for you to
understand. In our example, the cluster consists of 3 brokers. There is only
one topic that is spanned over all three brokers. The topic contains three
partitions with replicas across other brokers. One of the partitions acts as a
leader (marked in red color). Producers and consumers will connect only to
these partitions.

Let us summarize the various steps involved in the Kafka
system:

- Producers send messages on a topic. If the producer specifies a key then the messages with the same key will be stored in the same partition. Otherwise, the broker will decide where to store the message.
- Once Kafka receives the message, it is replicated across various replicas.
- Consumer subscribes to a specific topic. That means when Kafka receives any message on the subscribed topic, that message is passed to the consumer.
- The consumer receives the packet and sends back the acknowledgment to Kafka. Kafka updates the offset to this new value.
- If consumer crashes before sending the acknowledgement, and _auto_commit_interval_ also didn't lapse, then same message will be sent again to consumer.
- Consumer also has the option to rewind/skip to the desired offset of a topic at any time and read all the subsequent messages.

## Summary

With this let's conclude our today's discussion here. I'd to edit this article
multiple times just to make sure that it's easy to understand. Still, it has
the scope of language improvement ðŸ˜‰. However, it's too late now and I'm
exhausted. So let's keep it here.

In our next article, we'll discuss some
essential Kafka operations such as how to create a broker, set up partitions,
initiate communication between producer and consumer, and many more. Till then
enjoy your time ðŸŒƒ
